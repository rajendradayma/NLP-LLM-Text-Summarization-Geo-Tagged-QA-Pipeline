# -*- coding: utf-8 -*-
"""Text Summarization_Main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mownP-1zs5zZlFgOW_8TiZT3MF-qahqf

*THIS FIRST APPROACH
"""

!pip install crewai

# Install required packages
!pip install -q kaggle transformers datasets sentencepiece spacy geopy pandas tqdm
!python -m spdown spacy download en_core_web_lg -q

import os
import pandas as pd
from transformers import pipeline
import spacy
from geopy.geocoders import Nominatim
from tqdm import tqdm
import torch # Import the torch module

# 1. SET UP KAGGLE API (one-time setup)
# Create the kaggle.json file with your API credentials
import json
api_token = {"username":"rajendradayma","key":"6e3c5a35fe32480aef5fb51a659392d3"}  # Replace with your actual credentials

# Create .kaggle directory and write the credentials
os.makedirs('/root/.kaggle', exist_ok=True)
with open('/root/.kaggle/kaggle.json', 'w') as f:
    json.dump(api_token, f)
os.chmod('/root/.kaggle/kaggle.json', 600)

# 2. DOWNLOAD DATASET DIRECTLY
!kaggle datasets download -d gowrishankarp/newspaper-text-summarization-cnn-dailymail
!unzip -o newspaper-text-summarization-cnn-dailymail.zip

# 3. LOAD DATA
train_df = pd.read_csv('/content/cnn_dailymail/test.csv')
print(f"Dataset loaded with {len(train_df)} articles")
print(train_df.head(2))

# 4. INITIALIZE MODELS
summarizer = pipeline("summarization",
                     model="google/pegasus-cnn_dailymail",
                     device=0 if torch.cuda.is_available() else -1) # Now torch is defined and can be used
if not spacy.util.is_package("en_core_web_lg"):
    !python -m spacy download en_core_web_lg
nlp = spacy.load("en_core_web_lg")
geolocator = Nominatim(user_agent="geo_news_app")

# 5. PROCESSING FUNCTION
def process_article(text):
    # Summarization
    summary = summarizer(text, max_length=130, min_length=30)[0]['summary_text']

    # Location extraction
    doc = nlp(text)
    locations = list(set(ent.text for ent in doc.ents if ent.label_ in ("GPE", "LOC")))

    # Geocoding (with error handling)
    geo_data = {}
    for loc in locations:
        try:
            location = geolocator.geocode(loc)
            if location:
                geo_data[loc] = (location.latitude, location.longitude)
        except:
            continue

    return summary, locations, geo_data

# 6. PROCESS SAMPLE ARTICLES
results = []
sample_size = 5  # Adjust as needed

for i, row in tqdm(train_df.iloc[:sample_size].iterrows(), total=sample_size):
    summary, locations, geo_data = process_article(row['article'])
    results.append({
        'id': row['id'],
        'summary': summary,
        'locations': locations,
        'coordinates': geo_data
    })

# 7. SAVE RESULTS
output_df = pd.DataFrame(results)
output_df.to_csv('geo_summaries.csv', index=False)
print("Processing complete. Results saved to geo_summaries.csv")

"""# THIS SECOND APPROACH"""

!pip install transformers torch spacy geopy pandas tqdm
!python -m spacy download en_core_web_sm

import pandas as pd
import spacy
from tqdm import tqdm
from geopy.geocoders import Nominatim
from transformers import pipeline

# Load models
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")
qa_pipeline = pipeline("question-answering", model="distilbert-base-cased-distilled-squad")
nlp = spacy.load("en_core_web_sm")
geolocator = Nominatim(user_agent="geo_pipeline")

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch

# Load model and tokenizer
model_name = "facebook/bart-large-cnn"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to("cuda" if torch.cuda.is_available() else "cpu")

def summarize_text(text, min_words=50, max_words=800, max_token_length=1024):
    try:
        # Skip too-short texts
        if len(text.split()) < min_words:
            return "Text too short to summarize."

        # Truncate too-long texts
        words = text.split()
        if len(words) > max_words:
            text = " ".join(words[:max_words])

        # Tokenize input safely
        inputs = tokenizer.encode(text, return_tensors="pt", truncation=True, max_length=max_token_length)
        inputs = inputs.to(model.device)

        # Generate summary
        summary_ids = model.generate(inputs, max_length=130, min_length=30, length_penalty=2.0, num_beams=4, early_stopping=True)
        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
        return summary

    except Exception as e:
        print(f"Summarization error: {e}")
        return "Summarization failed."

def extract_location(text):
    doc = nlp(text)
    return list(set([ent.text for ent in doc.ents if ent.label_ == "GPE"]))

def get_coordinates(location):
    try:
        loc = geolocator.geocode(location)
        if loc:
            return (loc.latitude, loc.longitude)
    except Exception as e:
        print(f"Geocoding error for location '{location}': {e}")
    return (None, None)

def answer_question(context, question="What is the article about?"):
    try:
        result = qa_pipeline(question=question, context=context)
        return result['answer']
    except Exception as e:
        print(f"QA error: {e}")
        return ""

df = pd.read_csv("/content/cnn_dailymail/test.csv")  # <-- Replace if filename is different
df = df.head(100)

tqdm.pandas()
results = []

for _, row in tqdm(df.iterrows(), total=len(df)):
    article = row.get("article", "")
    if not article: continue

    summary = summarize_text(article)
    locations = extract_location(article)
    lat, lon = (None, None)
    if locations:
        lat, lon = get_coordinates(locations[0])

    answer = answer_question(article)

    results.append({
        "original_text": article,
        "summary": summary,
        "qa_answer": answer,
        "location": locations[0] if locations else None,
        "latitude": lat,
        "longitude": lon
    })

output_df = pd.DataFrame(results)
output_df.to_csv("final_output.csv", index=False)
print("âœ… Final output saved as final_output.csv")

from google.colab import files  # Import the 'files' object from google.colab

# ... your other code ...

files.download("final_output.csv")

